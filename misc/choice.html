<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CHOICE</title>
    <link rel="stylesheet" href="../styles/style.css">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Libre+Franklin:ital,wght@1,300&family=Oswald&family=Titillium+Web&display=swap"
        rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Kdam+Thmor+Pro&display=swap" rel="stylesheet">

</head>

<body background="C:\Users\chldp\OneDrive\바탕 화면\Website\images\background.jpg">


    <header class="header">

        <div class="header__logo">
            <a href="#">
                <h1>AI SENSOR TECHNOLOGY &
                    AUTONOMOUS VEHICLES</h1>
            </a>
        </div>

        <div class="header__gnb">
            <ul>
                <li><a href="../index.html">HOME</a></li>
                <li><a href="topic.html">TOPIC</a></li>
                <li><a href="opportunity.html">OPPORTUNITY</a></li>
                <li><a href="choice.html">CHOICE</a></li>
                <li><a href="risk.html">RISK</a></li>
                <li><a href="ethic.html">ETHICAL REFLECTIONS</a></li>
                <li><a href="references.html">REFERENCE</a></li>
            </ul>
        </div>
    </header>

    <Choice class="Choice">

        <div class="Choice" style='text-indent:15px'>
            <h1>The secret behind self-driving cars, safer than humans, lies in the Three Sensors
            </h1>
            <p>Human external stimuli are received from the five senses (sight, hearing, smell, taste, and touch). This
                stimulus is transmitted to the central nervous system through electrical signals, and through it, it
                thinks and makes decisions. Unlike traditional vehicles, self-driving cars have to think and drive
                themselves like humans. The operating principle of autonomous vehicles is similar to that of humans or
                animals. Just as humans feel a sense from neurons, sensors that are basically equipped with cars
                recognize surrounding environmental factors. An artificial intelligence (AI) computer (PC) in a vehicle
                controls a vehicle with information collected from a sensor. When the controller commands each part of
                the car, the car moves on its own, just as humans move on two feet.
            </p>
            <p>As such, the basis of thoughts and movements is information collected through sensory organs. Humans
                think and act because they read type with their eyes and hear voices with their ears. Therefore,
                accurate information collection is very important. This is because if the collected information is
                inaccurate, the judgment of the central nervous system and the resulting movement are likely to be
                wrong. This is why self-driving cars need advanced sensing capabilities to safely operate on their own
                without human help.
            </p>
        </div>

    </Choice>
    <Keysensors class="Keysensors">

        <div class="Keysensors" style='text-indent:15px'>
            <h1>Self-driving car key sensors, a camera, radar, and lidar</h1>
            <img src="/Website/images/sensorstr.jpg">

            <p>Recognition of the surrounding environment of self-driving cars begins largely with three sensors,
                including cameras, radars, and lidars. Just as humans have different roles in their eyes, nose, mouth,
                and ears, each sensor has different specialised fields. First of all, camera sensors are used to
                identify surrounding objects while classifying them into people, objects, and vehicles. However, it is
                difficult to identify certain objects in dark spaces or bad weather situations. It is different from the
                past when cameras were used only for lane recognition. Only 'Mobil Eye', an Israeli camera algorithm
                company, has commercialised the driver assistance device (DAS) by using the distance from the car in
                front and the time of arrival. Recently, Tesla and Hyundai Mobis have succeeded in developing camera
                technologies that recognize curbstones, gravel, and grass, and are mass-producing them. In other words,
                the recognition function, which was limited to the plane, has become possible to identify protruding
                structures.
            </p>
            <p>In the case of radar, which was originally developed for military purposes, it is a sensor that extracts
                information such as distance, speed, and direction from surrounding objects based on signals that emit
                electromagnetic waves and return. It also performs well regardless of weather or time. Radar is still
                applied to various advanced driver assistance system (ADAS) technologies such as emergency automatic
                braking devices and smart cruise control, as it can detect all short, medium, and long distances
                depending on frequency.
            </p>
            <p>The lower the frequency is of a long wavelength, the longer the distance to reach the radio waves of the
                relatively same output, while the accuracy decreases. Due to this characteristic, long-distance radar
                sensors use a low-frequency 77 gigahertz (GHz) band. The bandwidth of short-range radar sensors that
                require clearer information uses the 79GHz band. Long-range radars are confirmed to be more than 150 to
                200 metres, but the angle of view is around 40 degrees. Short-range radars detect distances within 100
                metres, but the angle of view is more than 100 degrees.
            </p>

            <p>The lidar sends and receives lasers (light) to and from objects to create a three-dimensional map. It
                mainly uses a short wavelength of 905 nanometers (nm) and has much more precise spatial resolution than
                radar. In addition, it is less affected by performance even in a ring environment where light is
                insufficient as its own light source. Therefore, it can be used even in environments that cameras and
                radars do not detect. Recently, a technology that can recognize a much wider space by increasing the
                output to 1,550 nm has also been developed. It is not absorbed by the human eye and has little
                interference from sunlight, so it has excellent stability.
            </p>
            <p>Lyda, whose operation is similar to that of humans, is called the "eye of an autonomous vehicle." A
                person calculates the distance between the left eye and the right eye, and the distance between the
                object seen in the left eye and the object seen in the right eye to determine the distance from the
                actual target object. The lidar can accurately grasp distance information by exchanging dozens of lasers
                per second with surrounding objects. This plays an important role in increasing the trust of the
                autonomous driving system. However, in the case of expensive lidar, the accuracy is inferior in bad
                weather as much as light is used.
            </p>

        </div>
    </Keysensors>

    <Fusion class="Fusion">

        <div class="Fusion" style='text-indent:15px'>
            <h1>" Camera vs Sensor Fusion "</h1>
            <p>In this way, the characteristics of each sensory institution should be properly demonstrated for the safe
                and accurate operation of autonomous vehicles. Since radar alone cannot accurately identify objects, and
                cameras still have limitations in accurately determining perspective by themselves, their parts must
                function according to their roles. Research and development of these various sensing technologies are
                necessary and sufficient conditions for securing future competitiveness in the automobile industry.</p>
            <p>The automobile industry is also focusing on developing software (SW) in order to fully incorporate this
                technology into actual driving. 'Sensor Fusion' is a technology that allows autonomous driving by
                combining information recognized by each sensor into one. It complements the strengths and weaknesses of
                each technology such as cameras, radar, and lidar. Recently, with the addition of AI technology,
                automobile systems design vehicle driving algorithms by fusing each sensing information like a human
                brain.</p>
            <p>Some companies are developing autonomous driving technology using only one sensor instead of sensor
                fusion technology in terms of price. Tesla, a U.S. electric vehicle company, is a representative
                example. Tesla CEO Elon Musk declared that he would build an autonomous car using only camera sensors
                because the price of the lidar is expensive and the power consumption is high. If eight cameras are
                photographed in three dimensions, it is possible to measure the shape and distance of an object and
                drive autonomously. Rather, CEO Musk believes that radar reduces precision due to a lot of distorted
                data.
            </p>
            <p>However, most self-driving cars still use various sensors. Google's Waymo, a leader in self-driving, uses
                radar and cameras together, focusing on lidar. Volvo is going to install Lumina, a Lidar sensor
                manufacturer, along with cameras and radar on next-generation electric vehicles that will be released
                next year. Audi first used LIDA from Valleo, a sensor company, on the A8 in 2017. Hyundai Motor is also
                planning to install two lidars in the Genesis G90, which will be released next year.
            </p>
        </div>
    </Fusion>



    <Foreign class="Foreign">

        <div class="Foreign" style='text-indent:15px'>
            <h1>What if there is a foreign object on the autonomous vehicle sensor?
            </h1>
            <p>If the lidar sensor is stained with muddy water or bird excrement, it affects perception and requires a
                separate system to clean it up. Various systems for preventing external contamination of autonomous
                driving sensors have been studied. Since radar uses radio waves, it is not affected by external
                contamination such as muddy water, but sensors that are greatly affected by external contamination are
                mainly lidar and camera sensors.
            </p>
            <img src="/Website/images/ridarwiper.PNG">
            <p>Google Waymo added wipers to its LIDAR sensor in 2017 similar to regular cars’ window wipers. The lidar
                sensor is vulnerable to pollution because it uses light. Therefore, a system was proposed to remove
                various pollutants such as rain, dust, and bird excrement. It is necessary to protect the cameras
                because dirt or dust on the cameras also greatly affects the recognition performance of autonomous
                vehicles.
            </p>
            <img src="/Website/images/camerawash.jpg">
            <p>In particular, since the front camera plays an important role in recognizing lanes, the currently
                commercialised partial autonomous driving capacities place the front camera within the operating range
                of the wiper. It can be wiped off with a wiper when muddy water or dust is on it.
            </p>
        </div>
    </Foreign>


    <Tesla class="Tesla">

        <div class="Tesla" style='text-indent:15px'>
            <h1>Tesla Self-Driving Sensor System
            </h1>
            <img src="/Website/images/tesla.PNG">
            <p>Tesla's autonomous driving system uses a total of eight cameras. Three front, two side, and three rear
                camera systems are used, of which the most important is the front camera. It is designed to protect
                primarily through wipers and secondly to melt snow through heaters. The side camera is also protected by
                a heater.
            </p>
        </div>
    </Tesla>

    <Valleo class="Valleo">

        <div class="Valleo" style='text-indent:15px'>
            <h1>Valleo's Camera Protection System
            </h1>
            <img src="/Website/images/backcamera.JPG">
            <p>Valleo, a French auto parts company, introduced the EverView Centricam in 2018 to protect camera sensors.
                The system is designed to protect water-drop camera sensors, especially rear cameras. The transparent
                plastic cover surrounding the camera sensor rotates at high speed to remove water droplets, which is
                designed to compensate for situations in which water droplets may be stained and the recognition rate
                may decrease.
            </p>
            <p>As a result, there is no big problem with radars using radio waves, but various protection systems are
                being studied because light-based radars and cameras are greatly affected by external pollution such as
                dust and muddy water. This means that sensor technology must be much more advanced in order to be able
                to drive fully autonomously in snow, rain, and dust situations.
            </p>
        </div>
    </Valleo>


</body>


</html>